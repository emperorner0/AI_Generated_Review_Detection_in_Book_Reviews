{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Samuel Middleton<br>\n",
    "Project: Using Fine-Tuned GPT2 to Generate Book Reviews<br>\n",
    "Github: https://github.com/emperorner0 <br>\n",
    "Email: samuelmiddleton93@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Business-Case-and-Purpose\" data-toc-modified-id=\"Business-Case-and-Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Business Case and Purpose</a></span></li><li><span><a href=\"#Data-Access\" data-toc-modified-id=\"Data-Access-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Access</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#MongoDB\" data-toc-modified-id=\"MongoDB-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>MongoDB</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Exploration</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#PyTorch-Custom-Dataset\" data-toc-modified-id=\"PyTorch-Custom-Dataset-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>PyTorch Custom Dataset</a></span></li><li><span><a href=\"#GPT2\" data-toc-modified-id=\"GPT2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>GPT2</a></span></li><li><span><a href=\"#Model-Instantiation\" data-toc-modified-id=\"Model-Instantiation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Model Instantiation</a></span></li><li><span><a href=\"#Model-Tuning\" data-toc-modified-id=\"Model-Tuning-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Model Tuning</a></span></li></ul></li><li><span><a href=\"#Model-Saving\" data-toc-modified-id=\"Model-Saving-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Saving</a></span></li><li><span><a href=\"#Test-Review-Generation\" data-toc-modified-id=\"Test-Review-Generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Test Review Generation</a></span></li><li><span><a href=\"#Further-Readings\" data-toc-modified-id=\"Further-Readings-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Further Readings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Case and Purpose\n",
    "\n",
    "This is secondary\n",
    "\n",
    "Open marketplaces with semi-anonymous review platforms have a significant problem, bot generated reviews. This issue is exacerbated by the common availability of high powered, free pre-trained models that puts the strength of advanced NLP neural networks in to the hands of everyone. The double edged nature of these powerful models is that they're amazing for research and advancement, but they also put the powers of near human levels of context and speech generation into the hands of black-hat users. \n",
    "\n",
    "We are seeking to counter these nefarious individuals by using the very technology they would use. Using GPT2 fine-tuned to book reviews we seek to optimize fake review generation in order to use BERT to detect fake reviews in this proof of concept machine learning product.\n",
    "\n",
    "This is an ancillary notebook to a main notebook in this repository. See the `Classifier_with_BERT` notebook for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:59.295473Z",
     "start_time": "2020-12-04T22:51:59.262469Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\Nero_\\anaconda3\\envs\\learn-env2\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-38edf5cc8ff4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env2\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m' Error loading \"{}\" or one of its dependencies.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\Nero_\\anaconda3\\envs\\learn-env2\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import pipelines\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.578259Z",
     "start_time": "2020-12-04T22:51:27.352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrain and save model if true\n",
    "save = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access\n",
    "\n",
    "Initially we attempted to utilize 51 million book reviews as provided by [Julian McAuley](http://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local) in a single dataframe. Due to the nature of dataframes, being memory inefficient and unable to stream data, we had to engineer a work-around. After further research we decided upon MongoDB. \n",
    "\n",
    "### MongoDB\n",
    "\n",
    "MongoDB is a simple document based database system that provides great flexibility in its expandability and extensibility. It does this by:\n",
    "\n",
    "- Offering JSON-like document storage, meaning data structure can change document to document\n",
    "\n",
    "- Easily map objects to application code\n",
    "\n",
    "- Ad hoc queries, indexing, and real-time aggregation.\n",
    "\n",
    "- Distributed at its core.\n",
    "\n",
    "- Free to use under the (SSPLv1 license)\n",
    "\n",
    "For our uses we were able to load in a 51 million `20+gb` `JSON` file up as a database. We were then able to aggregate and further sample the data so that we could feed a selection of the reviews into our model for fine tuning. \n",
    "\n",
    "Thus in the end we ended with a corpus of 50,000 reviews on which to train our `GPT2` model for review text generation. We chose not to push the number further due to lack of computer resources. Were we working with a distributed network architecture we could've easily expanded the corpus size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.579258Z",
     "start_time": "2020-12-04T22:51:27.354Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient() # Instantiate local PyMongo client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.580257Z",
     "start_time": "2020-12-04T22:51:27.356Z"
    }
   },
   "outputs": [],
   "source": [
    "db = client['local'] # Access database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.581258Z",
     "start_time": "2020-12-04T22:51:27.359Z"
    }
   },
   "outputs": [],
   "source": [
    "collection = db['reviewdata'] # Access collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.582261Z",
     "start_time": "2020-12-04T22:51:27.361Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use sample aggregation to pull 50k random review without replacement\n",
    "reviews = list(collection.aggregate([{ \"$sample\": {\"size\":50000}}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.583258Z",
     "start_time": "2020-12-04T22:51:27.370Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.583258Z",
     "start_time": "2020-12-04T22:51:27.374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the 50k reviews in a dataframe\n",
    "reviews_data = pd.DataFrame(reviews)['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.584260Z",
     "start_time": "2020-12-04T22:51:27.378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop any duplicates\n",
    "review_data = reviews_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.585259Z",
     "start_time": "2020-12-04T22:51:27.380Z"
    }
   },
   "outputs": [],
   "source": [
    "review_data # Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.586259Z",
     "start_time": "2020-12-04T22:51:27.383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop any null values added\n",
    "review_data.isna().sum()\n",
    "review_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.587259Z",
     "start_time": "2020-12-04T22:51:27.386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Maintain a clean copy of the dataframe\n",
    "reviews = review_data.copy()\n",
    "reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.588259Z",
     "start_time": "2020-12-04T22:51:27.388Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    # Label broadcasting and saving\n",
    "    review_data['label'] = 0\n",
    "    review_data.to_csv('Data/realreviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Our particular model of `GPT2` maintained the default token length at 1024 tokens, though to be safe we took reviews only under 768 tokens in length. So first we look at the distribution of token lengths in the reviews by breaking them down into tokens and observing the length in a list.\n",
    "\n",
    "Here we also instantiate our `GPT2` tokenizer provided by the Hugging Face transformers library. We are passing in custom tokens for beginning, ending, and padding sentences. \n",
    "\n",
    "We also notate that the average review length is 117 words, and the maximum length of a review is a novel at 6499 words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.588259Z",
     "start_time": "2020-12-04T22:51:27.393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize all of the reviews in the dataframe and get their length\n",
    "reviewlen = []\n",
    "for review in tqdm(reviews):\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    reviewlen.append(len(tokens))\n",
    "    \n",
    "reviewlen = np.array(reviewlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.590259Z",
     "start_time": "2020-12-04T22:51:27.397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting review length and average review length\n",
    "train_av = round(np.average(reviewlen),3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[8,6])\n",
    "plt.title('Test Histogram with Average Length')\n",
    "plt.hist(pd.Series(reviewlen), bins=30)\n",
    "ax.axvline(train_av, color ='red', lw = 2, alpha = 0.75) \n",
    "plt.legend(['Average Length: {}'.format(train_av), 'Test Histogram'])\n",
    "plt.savefig('Images/testlengen.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.592261Z",
     "start_time": "2020-12-04T22:51:27.399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting number of reviews over 768 tokens\n",
    "print(f\"Precent above 768 tokens: {round(len(reviewlen[reviewlen > 768])/len(reviewlen)*100, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.593264Z",
     "start_time": "2020-12-04T22:51:27.401Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Average review length: {} words.'.format(round(np.average(reviewlen), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.595262Z",
     "start_time": "2020-12-04T22:51:27.403Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Max review length: {} words.'.format(np.max(reviewlen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.597261Z",
     "start_time": "2020-12-04T22:51:27.405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize for modeling using GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2',\n",
    "                                          bos_token='<|sot|>', eos_token='<|eot|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.598259Z",
     "start_time": "2020-12-04T22:51:27.407Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Beginning of sentence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"End of sentence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"Padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## PyTorch Custom Dataset\n",
    "\n",
    "PyTorch has a Dataset inheritable class that can be used with the PyTorch framework. The Dataset inheritable class represents a Python iterable over a dataset that supports map-style or iterable-style datasets.\n",
    "\n",
    "* **Map-Style** - Represents a map of Key-Value pairs to data samples within the dataset.\n",
    "* **Iterable-Style** - Represents an iterable dataset like that which could be streamed from a database, remote server, or even generated in real-time.\n",
    "\n",
    "This uses the `__getitem__` method to implement data retrieval and is therefore a map-style dataset. The `__getitem__` method pulls a sequence to feed token sequences into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.599261Z",
     "start_time": "2020-12-04T22:51:27.409Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT_Finetune_Dataset(Dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes an iterable of reviews and transforms them into a PyTorch GPT2 dataset\n",
    "    Input:\n",
    "        txt_list[iterable] - List of reviews to use as dataset\n",
    "        Tokenizer[Instantiated Tokenizer] - Transformers library tokenizer to tokenize text\n",
    "        GPT2_type[String][Optional] - Default: 'gpt2'\n",
    "        Max_length[Int][Optional] - Max length of tokens put out by dataset\n",
    "    Returns:\n",
    "        Input sequences, attention mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=200):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input = []\n",
    "        self.attn = []\n",
    "        # Cycles through the iterable of txt_list \n",
    "        for txt in tqdm(txt_list):\n",
    "            # Encodes text adding padding, start, and end tokens.\n",
    "            encodings_dict = tokenizer('<|sot|>'+ txt +'<|eot|>',\n",
    "                                     truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "            self.input.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.attn[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.600260Z",
     "start_time": "2020-12-04T22:51:27.413Z"
    }
   },
   "outputs": [],
   "source": [
    "data = GPT_Finetune_Dataset(reviews, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "`GPT2` is a distinct model architecture that was developed by OpenAI and based upon their original `GPT` model. \n",
    "\n",
    "`GPT2` is based on the Transformer architecture. I have covered the basis of Transformer architecture in my `Classifier_with_BERT` notebook therefore I am going to only cover what makes `GPT(2)` different from other NLP transfer learning architectures.\n",
    "\n",
    "`GPT2` differs from something like `BERT` in that it only uses the decoder side of the Encoder-Decoder part of Transformer architecture. \n",
    "\n",
    "It differs greatly from `BERT` in that is doesn't actually change the chose tokens to `[MASK]` but instead chooses to interfere with the self-attention calculation for the tokens of the right of the current position being calculated. This is **Masked Self-Attention**, as opposed to `BERT`'s **Self-Attention**. (Self-Attention has been covered in the aforementioned notebook, please refer to that notebook for further information on the concept.)\n",
    "\n",
    "The Decoder Stack that makes up the `GPT2` transformer architecture contains decoders that are cells of masked self-attention layers and then a feed-forward neural network layer. These are stacked to produce the `GPT2` architecture.\n",
    "\n",
    "Before token sequences are passed tot he decoder stack they are first embedded into vocabularies and then position embedded. These embeddings are then passed up the decoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.601258Z",
     "start_time": "2020-12-04T22:51:27.415Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(data) * .7)\n",
    "test_size = len(data) - train_size\n",
    "\n",
    "train_set, test_set = random_split(data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.602263Z",
     "start_time": "2020-12-04T22:51:27.416Z"
    }
   },
   "outputs": [],
   "source": [
    "print('{} training samples'.format(train_size))\n",
    "print('{} test samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.603258Z",
     "start_time": "2020-12-04T22:51:27.418Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,  # The training set\n",
    "            sampler = RandomSampler(train_set), # Random sampler\n",
    "            batch_size = batch_size # Trains with this batch size for memory reasons\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_set, # The validation samples.\n",
    "            sampler = SequentialSampler(test_set), # Pull out batches sequentially since order doesn't matter\n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.603258Z",
     "start_time": "2020-12-04T22:51:27.420Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.605259Z",
     "start_time": "2020-12-04T22:51:27.421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get config\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "# Model instantiation\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "# Necessary because of the custom tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "# Setting seeds\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.607260Z",
     "start_time": "2020-12-04T22:51:27.423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting Parameters\n",
    "epochs = 5\n",
    "learning_rate = .00005\n",
    "warmup_steps = 50\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.608259Z",
     "start_time": "2020-12-04T22:51:27.424Z"
    }
   },
   "outputs": [],
   "source": [
    "#AdamW is a class from the huggingface library that schedules weights\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.609260Z",
     "start_time": "2020-12-04T22:51:27.426Z"
    }
   },
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Adjusts the learning rate as the model steps through\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.610260Z",
     "start_time": "2020-12-04T22:51:27.428Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    timestat = time.time()\n",
    "\n",
    "    stats = []\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # training loop\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "\n",
    "        # Setup time\n",
    "        timestat = time.time()\n",
    "\n",
    "        # Initiate loss\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # PyTorch training mode\n",
    "        model.train()\n",
    "\n",
    "        # Pull data from data loader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Pass the 3 dataloader outputs in device\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[0].to(device)\n",
    "            b_masks = batch[1].to(device)\n",
    "\n",
    "            # Zero gradients to prevent epoch contamination\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Grab model outputs\n",
    "            outputs = model(  b_input_ids,\n",
    "                              labels=b_labels, \n",
    "                              attention_mask = b_masks,\n",
    "                              token_type_ids=None\n",
    "                            )\n",
    "            # Pull loss from loss function\n",
    "            loss = outputs[0]  \n",
    "\n",
    "            # Detach loss from CUDA and add to total loss\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "            # Get sample every x batches.\n",
    "            if step % sample_every == 0 and not step == 0:\n",
    "                # Use initial time to get time stats\n",
    "                elapsed = time.time() - timestat\n",
    "                print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "                # Model into PyTorch eval mode\n",
    "                model.eval()\n",
    "\n",
    "                # Generate a sample\n",
    "                sample_outputs = model.generate(\n",
    "                                        bos_token_id=random.randint(1,30000),\n",
    "                                        do_sample=True,   \n",
    "                                        top_k=50, \n",
    "                                        max_length = 200,\n",
    "                                        top_p=0.95, \n",
    "                                        num_return_sequences=1\n",
    "                                    )\n",
    "                for i, sample_output in enumerate(sample_outputs): # print a sample\n",
    "                      print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "\n",
    "                # Convert model back to training mode\n",
    "                model.train()\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Scheduler step\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = time.time() - timestat\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "        # Testing loop\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_labels = batch[0].to(device)\n",
    "            b_masks = batch[1].to(device)\n",
    "\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                outputs  = model(b_input_ids, \n",
    "                                 attention_mask = b_masks,\n",
    "                                labels=b_labels)\n",
    "\n",
    "                loss = outputs[0]  \n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            total_eval_loss += batch_loss        \n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "\n",
    "        validation_time = time.time() - timestat \n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.611258Z",
     "start_time": "2020-12-04T22:51:27.429Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    # Convert stats dict into a dataframe  \n",
    "    training_df = pd.DataFrame(data=stats)\n",
    "    training_df = training_df.set_index('epoch')\n",
    "    training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.611258Z",
     "start_time": "2020-12-04T22:51:27.431Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    # Seaborn plot for training and Valid loss\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    plt.plot(training_df['Training Loss'], 'r-o', label=\"Training\")\n",
    "    plt.plot(training_df['Valid. Loss'], 'b-o', label=\"Validation\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.xticks([1, 2, 3, 4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving\n",
    "\n",
    "Model saving is an integral part of model deployment and the fine tuning process. Having used the pre-trained model made available from the Hugging Face Tranformers library we are able to work with their `save_pretrained()` and `load_pretrained()` methods, but could just as easily use native PyTorch models save and load functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.612259Z",
     "start_time": "2020-12-04T22:51:27.433Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    # Save params\n",
    "    params = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.613261Z",
     "start_time": "2020-12-04T22:51:27.434Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    # Check params of model to save\n",
    "    for param in params:\n",
    "        print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.614259Z",
     "start_time": "2020-12-04T22:51:27.436Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    # Create directory if not available \n",
    "    output_dir = './model_save/'\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.615260Z",
     "start_time": "2020-12-04T22:51:27.438Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    print(\"Saving model to {}\".format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.616258Z",
     "start_time": "2020-12-04T22:51:27.440Z"
    }
   },
   "outputs": [],
   "source": [
    "if save:\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.617259Z",
     "start_time": "2020-12-04T22:51:27.442Z"
    }
   },
   "outputs": [],
   "source": [
    "if save is False:\n",
    "    # Initiate model from fine-tuned model\n",
    "    gpt_config = GPT2Config.from_pretrained('model_save/', output_hidden_states=False)\n",
    "    model = GPT2LMHeadModel.from_pretrained('model_save/', config=gpt_config)\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('model_save/',\n",
    "                                            vocab_file='model_save/vocab.json', merges_file='model_save/merges.txt',\n",
    "                                            bos_token='<|sot|>', eos_token='<|eot|>', pad_token='<|pad|>')\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Review Generation\n",
    "\n",
    "Text generation is one of the main purposes of the `GPT2` model. We take a random sample of the reviews on which we trained the initial model. These are then broken down to a start of sentence token and then a 4 length token sequence. This sequence is the **Prompt**. The prompt is required for the model. The model will take the prompt and then use it to generate context based upon it. The 4 token (5, if the start of sentence token is counter) sequence can then be tokenized and fed into the model so that it can generate responses to the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.618260Z",
     "start_time": "2020-12-04T22:51:27.443Z"
    }
   },
   "outputs": [],
   "source": [
    "test2 = random.sample(set(reviews.index), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.619260Z",
     "start_time": "2020-12-04T22:51:27.446Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for x in test2:\n",
    "    try:\n",
    "        prompts.append(\"<|sot|> \" + ' '.join(reviews[x].split()[:4]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.620259Z",
     "start_time": "2020-12-04T22:51:27.450Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.621259Z",
     "start_time": "2020-12-04T22:51:27.452Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Encode prompts and save to list\n",
    "gen_prompt = []\n",
    "for prompt in prompts:\n",
    "    gen_prompt.append(torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.623259Z",
     "start_time": "2020-12-04T22:51:27.460Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewlist = [] # Empty list to store reviews\n",
    "t = time.time() # Time save  \n",
    "\n",
    "# Loop to take prompt (initial 4 words) from list of real reviews\n",
    "for x, promp in enumerate(gen_prompt):\n",
    "    t1 = time.time() # loop time\n",
    "    print('---------------------------------')\n",
    "    print('''{}: The prompt is \"{}\"'''.format(x+1, prompts[x])) # Pretty output\n",
    "    print('---------------------------------')\n",
    "    \n",
    "    # Review generation block\n",
    "    sample_outputs = model.generate(\n",
    "                                    promp, # Prompt\n",
    "                                    bos_token_id= random.randint(1, 100000), # Token randomization\n",
    "                                    do_sample=True,   \n",
    "                                    top_k=30, # Prevent the review from repeating\n",
    "                                    min_length=20, # Min token length\n",
    "                                    max_length = 250, # max token length\n",
    "                                    top_p=0.95, \n",
    "                                    num_return_sequences=20 # number to return\n",
    "                                    )\n",
    "    \n",
    "    # Output reviews and save them to list\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "      print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "      reviewlist.append(\"{}\".format(tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "    time_per_gen = time.time() - t1\n",
    "    print('This generation took {} seconds'.format(round(time_per_gen, 3)))\n",
    "totes = time.time() - t\n",
    "print('Total time was {}'.format(str(timedelta(seconds = totes))))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.624259Z",
     "start_time": "2020-12-04T22:51:27.464Z"
    }
   },
   "outputs": [],
   "source": [
    "reviewlist # Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.625260Z",
     "start_time": "2020-12-04T22:51:27.467Z"
    }
   },
   "outputs": [],
   "source": [
    "remove = False\n",
    "if remove:\n",
    "    # Remove limit characters\n",
    "    testlist = []\n",
    "    removedict = [('\\n', ' '), (\"\\\\\", '')]\n",
    "    for review in reviewlist:\n",
    "        reveiw = re.sub('\\n', ' ', review)\n",
    "        testlist.append(re.sub(\"\\'\", \"\", reveiw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.626261Z",
     "start_time": "2020-12-04T22:51:27.469Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Real review prompt:{reviews[test2[0]]}\")\n",
    "print('')\n",
    "print(f\"Fake review from prompt:{reviewlist[6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.627261Z",
     "start_time": "2020-12-04T22:51:27.471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save to Dataframe from list\n",
    "reviewdf = pd.DataFrame(testlist, columns=['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.628259Z",
     "start_time": "2020-12-04T22:51:27.474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resample and shuffle\n",
    "reviewdf = reviewdf.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.629260Z",
     "start_time": "2020-12-04T22:51:27.476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Broadcast label\n",
    "reviewdf['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T22:51:32.630258Z",
     "start_time": "2020-12-04T22:51:27.477Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "if save:\n",
    "    reviewdf.to_csv('Data/reviewseries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Readings\n",
    "\n",
    "Further GPT2 explanations:\n",
    "\n",
    "http://jalammar.github.io/illustrated-gpt2/\n",
    "\n",
    "http://humanssingularity.com/gpt2sampling/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env2",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.455px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
